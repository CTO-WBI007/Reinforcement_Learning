Chapter 2.Value-Based reinforcement learning

1) DQN (deep Q network) : use a neural link network to approximate the Q* function, a prediction to future occurence

// Q(st,at;w) is a neural network parameterized by w;

construction of DQN : state -> conv -> feature vector -> dense -> Q* function //hence we can use DQN to play game

but how to train DQN? we use TD algorithm

2) Temporal Difference Learning (TDL) : make ITERATION !
procedure : 1. make a prediction q = Q(w);
2.get the target y;
3.Loss L = 1/2 * (q - y) ^ 2;
4.gradient : L对w求偏导
5.gradient descen : update model parameter w;

example : driving from NYC to Atlanta, estimated 1000 min; but when I arrive DC, actually spend 300 min, now making another prediction, now is 600 min from DC to Atlanta, 300 + 600 = 900 min
we call 900 min TD target; // 900 is a more reliable prediction than 1000;

TD error : 400 - 300 = 100; we want to diminish TD error;

In DRL: Q(st,at;w)//before departure = rt + y * Q(st+1,at+1;w) //after departure, right parameter is more accurate


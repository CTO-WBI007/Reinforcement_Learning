//1.概率论知识：随机变量，概率密度函数，随机抽样

Hint:there is a lot of terminologies in this course, so if u want to learn the course well, understand those jargons. Let's get started;

//2. 
1) state : this frame 

2)action 
3)agent 
4)policy : a kind of function,according to the state, make decision. It is the probability of taking action A = a given a certain state s;
action A can be random  
5)reward : the definition of reward is crucial. 
6)state transition : can be certain or random, usually it is deemed as random. Randomness is from the environment.
7)agent environment interaction : state -> agent -> env -> reward;
State transitions and actions both have randomness.

8)return(aka cumulative future reward) : future reward has less weight than present reawrd;
9)discounted return: pol; tuning hyper-parameter
10)value functions Q:action value function for policy PI / optimal action-value funtion Q* = max(PI) Q / state value function V(PI);
if actions are discrete, use sigma; if continuous, use calculus

*for a certain policy PI，value function Q(s,a) evaluates how good it is for an agent to pick action a while being in state s;

*for a fixed policy PI，value function V(s) evaluates how good the situation is in state s;



